WSL2:

# Add the NVIDIA CUDA repo
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/cuda-ubuntu2404.pin
sudo mv cuda-ubuntu2404.pin /etc/apt/preferences.d/cuda-repository-pin-600

sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/3bf863cc.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/x86_64/ /"

sudo apt update
sudo apt install -y cuda


sudo apt update
sudo apt install -y build-essential cmake git
sudo apt install -y cuda-toolkit
sudo apt install nvidia-cuda-toolkit


git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp



-- Build with CUDA
cmake -B build -DGGML_CUDA=ON
-- Testing without CUDA
cmake -B build -DGGML_BUILD_SERVER=ON

cmake --build build --config Release

-- Get models
mkdir -p ~/models
cd ~/models
export HUGGINGFACE_TOKEN=hf_xxxYourTokenHerexxx
wget --header="Authorization: Bearer $HUGGINGFACE_TOKEN" \
     https://huggingface.co/ggml-org/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-mxfp4.gguf



-- Test, with CUDA
./build/bin/llama-server \
  -m ~/models/gpt-oss-120b-mxfp4.gguf \
  -c 65536 \
  --gpu-layers 12 \
  --host 0.0.0.0 \
  --port 8081
  --threads 16

-- Test, without CUDA
./build/bin/llama-server \
  -m ~/models/gpt-oss-20b-mxfp4.gguf \
  -c 32768 \
  --host 0.0.0.0 \
  --port 8081
  
  
./build/bin/llama-server \
  -m /models/gpt-oss-120b-mxfp4.gguf \
  -c 98304 \
  --gpu-layers 50 \
  --threads 16 \
  --batch-size 512 \
  --host 0.0.0.0 \
  --port 8081
  --threads <<num_cores>>
  
  
  
  
Proper Ubuntu 24.04:
-- 1 Update and Install Essentials
sudo apt update
sudo apt upgrade -y
sudo apt install -y build-essential cmake git wget curl unzip

-- 2 Remove Any NVIDIA/CUDA Repo You Might Have, just in case the previous CUDA repo is present:
sudo rm -f /etc/apt/sources.list.d/*cuda*
sudo rm -f /etc/apt/preferences.d/cuda*
sudo apt-key del 3bf863cc 2>/dev/null || true
sudo apt update

-- 3 Install Ubuntuâ€™s Recommended NVIDIA Driver (550+). Check recommended driver:
ubuntu-drivers devices
-- Install it (most likely nvidia-driver-550):
sudo apt install -y nvidia-driver-550
-- Reboot:
sudo reboot
-- Verify:
nvidia-smi
You should see your L40S GPU with driver 550.

-- 4 Install CUDA Toolkit ONLY (No Driver). Ubuntu package, version 12.4:
sudo apt install -y cuda-toolkit-12-4
-- Verify:
nvcc --version

-- 5 Install Dependencies for llama.cpp
sudo apt install -y libopenblas-dev libomp-dev

-- 6 Clone and Build llama.cpp with CUDA
cd ~
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release

-- This will produce llama-server and llama-cli with CUDA support in ./build/bin

-- 7 Run llama.cpp Server

Example for GPT-OSS 20B (adjust model path and context):

./build/bin/llama-server \
  -m /path/to/gpt-oss-20b-mxfp4.gguf \
  -c 32768 \
  --gpu-layers 60 \
  --host 0.0.0.0 \
  --port 8081

-- Notes

Do not install cuda, cuda-drivers, or nvidia-cuda-toolkit from NVIDIA repo
nvidia-driver-550 + cuda-toolkit-12-4 is fully compatible with L40S
If you need bleeding-edge driver features, you can upgrade later, but stick to Ubuntu packages