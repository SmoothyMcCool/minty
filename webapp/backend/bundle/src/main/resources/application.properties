output.directory=d:/projects/Minty

docFileStore=${output.directory}/working/docs
tempFileStore=${output.directory}/working/temp
taskLibrary=${output.directory}/working/taskLib
pythonScripts=${output.directory}/working/python

#applicationDbUrl=jdbc:mariadb://localhost:3306/Minty
applicationDbUrl=jdbc:mariadb://192.168.2.54:3306/Minty
applicationDbPort=3306
applicationDbName=Minty
applicationDbUser=MintyUser
applicationDbPassword=hothamcakes

ollamaUri=http://localhost:11434
defaultModel=llama3.2
ollamaChatModels=gemma3:12b,gpt-oss:20b,gpt-oss:120b,llama4,llama3.2,codellama,tinyllama,devstral
conversationNamingModel=llama3.2
diagrammingModel=gpt-oss:20b
ollamaEmbeddingModel=nomic-embed-text
ollamaSummarizingModel=gpt-oss:20b
chatMemoryDepth=20
# The maximum amount of time, in minutes, that the Ollama API will wait for a synchronous response
ollamaApiTimeoutMinutes=20
keywordsPerDocument=5
# The maximum amount of time, in minutes, for an async response to complete. This is used for stream responses from the AI.
asyncResponseTimeout=20
# The maximum number of requests that can ever be queued before we start dropping requests to the LLM.
llmMaxRequests=100
# The number of documents to retrieve when doing RAG. (topK)
documentRagLimit=2

# Default and maximum number of threads for task execution. AI processing does not run in these threads
defaultTaskThreads=5
maximumTaskThreads=20
# Default and maximum number of threads for LLM queries.
defaultLlmThreads=2
maximumLlmThreads=2
# Default and maximum number of threads for document processing.
defaultDocumentThreads=1
maximumDocumentThreads=2

secret=TheColonelsSecretRecipeIsChickenGreaseSalt765346586723457683244567

# System default values for various Tasks
Get\ Confluence\ Pages\:\:Base\ URL=http://confluence.ca
Get\ Confluence\ Pages\:\:Use\ Bearer\ Authorization=true
